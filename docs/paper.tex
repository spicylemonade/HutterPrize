\documentclass[11pt]{article}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{microtype}
\title{Self-contained enwik9 Compression for the Hutter Prize}
\author{Your Name\\Your Affiliation}
\date{\today}
\begin{document}
\maketitle

\begin{abstract}
We describe a compressor and self-extracting decompressor for the Hutter Prize dataset (enwik9). The goal is to minimize the total size S = S1 + S2, where S1 is the compressor binary size and S2 is the produced self-extracting archive size, under strict constraints (single CPU core, <10GB RAM, <100GB disk, \textasciitilde{}\leq50h runtime), and to beat the current record L = 110{,}793{,}128 bytes as of October 2025. This document details the modeling, preprocessing, transforms, implementation, and evaluation protocol.
\end{abstract}

\section{Introduction}
The Hutter Prize incentivizes research into general-purpose compression and modeling of textual data. The enwik9 dataset comprises exactly $1{,}000{,}000{,}000$ bytes of the English Wikipedia dump (enwiki-20060303-pages-articles.xml). Our task is to build a compressor producing a self-contained archive that reproduces enwik9 bit-identically without network or external resources.

\section{Rules and Constraints}
We restate the essential rules: deliver two executables (compressor and self-extracting archive), total size $S = S1 + S2$, single-core runtime with limited memory/disk/time, and open-source publication with a 30-day comment period. The archive must regenerate enwik9 with no inputs.

\section{Method}
Describe modeling, preprocessing, reordering, tokenization, dictionary learning, entropy coding, and context mixing (if any). Explain all transforms applied and how they are reversed within the self-extracting archive without external resources. Provide rationale and ablations.

\section{Implementation}
Summarize system architecture, data pipeline, and engineering details. Specify OS, compiler, flags, and reproducibility steps. Ensure decompressor integrity: no file/network dependencies beyond the executable itself.

\section{Evaluation}
Detail the verification protocol: dataset acquisition, byte counts, build commands, runtime measurements (taskset, /usr/bin/time -v), integrity checks (cmp/sha1sum), and size calculations. Report S1, S2, S, and compare to record L and 1\% threshold.

\section{Results}
% Auto-included results if generated by scripts/report.sh
\IfFileExists{results.tex}{\input{results.tex}}{\emph{Generate results with \texttt{bash scripts/report.sh} to include metrics here.}}

\section{Discussion}
Analyze which components contributed most to gains. Discuss trade-offs: model size vs. archive size, preprocessing complexity vs. decompressor footprint, and generalization.

\section{Limitations and Future Work}
Note resource bottlenecks and planned improvements.

\section{Reproducibility}
List exact commands and environment. Include commit hash, compiler versions, and hardware details. Provide instructions compatible with the organizers' verification machine.

\section{Conclusion}
Summarize contributions and relation to the Hutter Prize objectives.

\paragraph{License} This work is released under an OSI license (see LICENSE). Source code is public per the prize requirement.

\end{document}
